{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference in the Poisson Generalized Linear Model\n",
    "\n",
    "**References:**\n",
    "- Chapter 16 of BDA3 contains background material on generalized linear models.\n",
    "- Chapter 7.1 of BDA3 introduces notation for model evaluation based on predictive log likelihoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Poisson GLM\n",
    "\n",
    "The Poisson distribution is a common model for count data with a single parameter $\\lambda \\in \\mathbb{R}_+$. Its pmf is,\n",
    "\n",
    "\\begin{align}\n",
    "\\Pr(y \\mid \\lambda) &= \\frac{1}{y!} e^{-\\lambda} \\lambda^y,\n",
    "\\end{align}\n",
    "\n",
    "for $y \\in \\mathbb{N}$. Its mean and variance are both equal to $\\lambda$.\n",
    "\n",
    "Suppose we have count observations $y_n \\in \\mathbb{N}$ along with covariates $x_n \\in \\mathbb{R}^P$. We construct a Poisson GLM by modeling the expected value as,\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[y_n \\mid x_n] = f(w^\\top x_n),\n",
    "\\end{align}\n",
    "\n",
    "with $w \\in \\mathbb{R}^P$ and $f: \\mathbb{R} \\to \\mathbb{R}_+$ is the mean function. The _canonical mean function_ is $f(a) = e^a$; equivalently, the canonical _link_ function is the logarithm.\n",
    "\n",
    "\n",
    "We assume a Gaussian prior on the weights $w$:\n",
    "$$\n",
    "w \\sim \\mathcal{N}(0, \\sigma^2 I),\n",
    "$$\n",
    "\n",
    "where $\\sigma^2 I$ is the covariance matrix.\n",
    "\n",
    "Derive the log joint probability,\n",
    " \n",
    "\\begin{align}\n",
    "\\mathcal{L}(w) &\\triangleq \\log p(\\{y_n\\}_{n=1}^N, w \\mid \\{x_n\\}_{n=1}^N, \\sigma^2) \\\\\n",
    "&= \\log p(w\\mid\\sigma^2) + \\sum_{n=1}^N\\log p(y_n \\mid w^Tx_n)\n",
    "\\end{align}\n",
    "\n",
    "If we replace $\\lambda = \\exp(w^Tx_n)$ in the Possion pmf we get\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(w) &= \\log\\mathcal{N}(w\\mid 0, \\sigma^2\\mathbf{I})+\\sum_{n=1}^N\\log\\left(\\frac{1}{y_n!}\\exp\\left\\{\\left \\langle y_n, w^Tx_n \\right \\rangle-\\exp(w^Tx_n)\\right\\}\\right)\\\\\n",
    "&= \\log\\mathcal{N}(w\\mid 0, \\sigma^2\\mathbf{I}) + \\sum_{n=1}^N\\left(\\left \\langle y_n, w^Tx_n \\right \\rangle - \\exp(w^Tx_n) - \\log (y_n!)\\right)\\\\\n",
    "&= -\\frac{w^Tw}{2\\sigma^2} + \\left \\langle \\sum_{n=1}^Ny_nx_n, w \\right \\rangle - \\sum_{n=1}^N\\exp(w^Tx_n) + C\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_w \\mathcal{L}(w) &= -\\nabla_w\\frac{w^Tw}{2\\sigma^2} +  \\nabla_w\\left \\langle \\sum_{n=1}^Ny_nx_n, w \\right \\rangle -  \\nabla_w\\sum_{n=1}^N\\exp(w^Tx_n)\\\\\n",
    "&= -\\frac{w}{\\sigma^2}+\\sum_{n=1}^Ny_nx_n-\\sum_{n=1}^Nx_n\\exp(w^Tx_n)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla^2_w \\mathcal{L}(w) &= -\\nabla_w\\frac{w}{\\sigma^2}+\\nabla_w\\sum_{n=1}^Ny_nx_n-\\nabla_w\\sum_{n=1}^Nx_n\\exp(w^Tx_n)\\\\\n",
    "&= -\\frac{I}{\\sigma^2}+0-\\sum_{n=1}^Nx_n\\nabla_w\\exp(w^Tx_n)\\\\\n",
    "&= -\\frac{I}{\\sigma^2}-\\sum_{n=1}^Nx_nx_n^T\\exp(w^Tx_n)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum a posterior (MAP) Estimation\n",
    "\n",
    "### Optimize to find the posterior mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.d01_data.dengue_data_api import DengueDataApi\n",
    "from src.d04_modeling.poisson_glm import PoissonGLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dda = DengueDataApi()\n",
    "x_train, x_validate, y_train, y_validate = dda.split_data()\n",
    "\n",
    "axs0 = sns.heatmap(y_train.unstack('weekofyear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma2 = 1.\n",
    "poisson_glm1 = PoissonGLM(x_train=x_train, y_train=y_train, sigma2=sigma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, w_hist = poisson_glm1.compute_posterior_mode()\n",
    "w_hist_df = pd.DataFrame(w_hist, columns=x_train.columns)\n",
    "w_hist_df['log_joint'] = w_hist_df.apply(lambda w: poisson_glm1.log_joint(y_train.values.reshape((-1,1)),\n",
    "                                                                          x_train.values, 1.,\n",
    "                                                                            w.values, sigma2), axis=1)\n",
    "w_hist_df.name = 'iter'\n",
    "axs1 = sns.lineplot(data=w_hist_df.iloc[1:].reset_index(), x=\"index\", y=\"log_joint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Approximation\n",
    "\n",
    "\n",
    "\n",
    "### Approximate the covariance at the mode\n",
    "\n",
    "Solve for $\\Sigma_{\\mathsf{MAP}} = -[\\nabla^2(\\mathcal{L}(w_{\\mathsf{MAP}})]^{-1}$. Plot the covariance matrix (e.g. with `imshow`). Don't forget to add a colorbar and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_glm1.compute_laplace_approximation()\n",
    "cov_map = poisson_glm1.get_cov_map()\n",
    "cov_map_df = pd.DataFrame(cov_map, index=x_train.columns, columns=x_train.columns)\n",
    "axs2 = sns.heatmap(cov_map_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior of the weights\n",
    "\n",
    "Plot the posterior mean of the weights for features $x_n$ (i.e. not including the bias term). Also plot 95% credible intervals around the mean by using two standard deviations of the marginal distribution of the weights. Note the diagonal of $\\Sigma_{\\mathsf{MAP}}$ gives the marginal variance of the posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_map = poisson_glm1.get_w_map()\n",
    "var_map = np.diagonal(cov_map)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "n = len(var_map)\n",
    "ax.errorbar(range(n), w_map, yerr=1.96*np.sqrt(var_map/n), marker='o', linestyle='none')\n",
    "ax.set_xlabel('Covariate')\n",
    "ax.set_xticks(range(n))\n",
    "ax.set_xticklabels(x_train.columns, rotation='vertical')\n",
    "ax.set_ylabel('$w$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation\n",
    "\n",
    "### Approximate the posterior predictive distribution of the rates\n",
    "\n",
    "We can draw many samples $w^{(s)}$ from the Laplace approximation of the posterior $p(w \\mid \\{x_n, y_n\\})$. Use those samples to approximate the posterior predictive distribution on the **test** dataset,\n",
    "\n",
    "\\begin{align}\n",
    "p(y_{n'}=k \\mid x_{n'}, \\{x_n, y_n\\}_{n=1}^N) &= \n",
    "\\int p(y_{n'} \\mid w, x_{n'}) \\, p(w \\mid \\{x_n, y_n\\}_{n=1}^N) \\, \\mathrm{d} w \\\\\n",
    "&\\approx \\frac{1}{S} \\sum_{s=1}^S p(y_{n'}=k \\mid w^{(s)}, x_{n'})\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "w^{(s)} &\\sim p(w \\mid \\{x_n, y_n\\}_{n=1}^N \\\\\n",
    "&\\approx \\mathcal{N}(w \\mid w_{\\mathsf{MAP}}, \\Sigma_{\\mathsf{MAP}})\n",
    "\\end{align}\n",
    "\n",
    "Visualize the posterior predictive distribution as an $K \\times N_{\\mathsf{test}}$ array where row corresponds to possible spike counts $k\\in \\{0,\\ldots, K\\}$. You can set $K=5$ for this problem. **Only show the first 100 columns (time bins), otherwise it's hard to see changes in the rate.**\n",
    "\n",
    "Overlay the actual spike counts for the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_joint, mae = poisson_glm1.validate_model(x_validate=x_validate, y_validate=y_validate)\n",
    "print(\"Log Joint Probability: %.2f\" % log_joint)\n",
    "print(\"MAE: %.6f\" % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Descomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate, y_train, y_validate = dda.split_data()\n",
    "\n",
    "u, s, vh = np.linalg.svd(x_train, full_matrices=True)\n",
    "num_components = 4\n",
    "new_features = [\"pc%i\" % i for i in range(num_components)]\n",
    "z_train = pd.DataFrame(np.dot(x_train, vh[:num_components, :].T), columns=new_features, index=x_train.index)\n",
    "z_validate = pd.DataFrame(np.dot(x_validate, vh[:num_components, :].T), columns=new_features, index=x_validate.index)\n",
    "\n",
    "sigma2 = 1.\n",
    "poisson_glm2 = PoissonGLM(x_train=z_train, y_train=y_train, sigma2=sigma2)\n",
    "\n",
    "poisson_glm2.compute_laplace_approximation()\n",
    "cov_map = poisson_glm2.get_cov_map()\n",
    "cov_map_df = pd.DataFrame(cov_map, index=z_train.columns, columns=z_train.columns)\n",
    "axs2 = sns.heatmap(cov_map_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_map = poisson_glm2.get_w_map()\n",
    "var_map = np.diagonal(cov_map)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "n = len(var_map)\n",
    "ax.errorbar(range(n), w_map, yerr=1.96*np.sqrt(var_map), marker='o', linestyle='none')\n",
    "ax.set_xlabel('Covariate')\n",
    "ax.set_xticks(range(n))\n",
    "ax.set_xticklabels(z_train.columns, rotation='vertical')\n",
    "ax.set_ylabel('$w$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_joint, mae = poisson_glm2.validate_model(x_validate=z_validate, y_validate=y_validate)\n",
    "print(\"Log Joint Probability: %.2f\" % log_joint)\n",
    "print(\"MAE: %.6f\" % mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dengai-env",
   "language": "python",
   "name": "dengai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
